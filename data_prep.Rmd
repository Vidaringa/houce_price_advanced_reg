---
title: "House Prices: Advanced Regression Techniques"
output:
        github_document:
                toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# Data analysis
library(tidyverse)
library(corrplot)
library(mice)     # For imputing missing values

# Modeling
library(tidymodels)
library(caret)

# Visualization
library(naniar)
library(visdat)
library(ComplexHeatmap)


training <- read_csv("train.csv") %>% janitor::clean_names()
testing <- read_csv("test.csv") %>% janitor::clean_names()

sale_price <- training$sale_price

train_id <- training$id
test_id <- testing$id

all <- bind_rows(training[,-81], testing)

```

# Introduction
In this blog I will walk through the steps I take in analyzing and modelling the data for this competition. This is the first Kaggle competition I participate in. You can read about it [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview). 


## First step
First I combine the training and the test set. I only do this because I have fix few variable that have missing values but with know reason, see the data description text file. After fixing the missing values I split the dataset back to training and test using the id's from both dataset. Since the reason of missingness is known and it's the same between training and test set there is no danger of lookahead bias.  

```{r, miss_train}
# Missing data according to data_description.txt file

# Various
all$pool_qc[is.na(all$pool_qc)] <- "no_pool"
all$misc_feature[is.na(all$misc_feature)] <- "none"
all$alley[is.na(all$alley)] <- "no_alley"
all$fence[is.na(all$fence)] <- "no_fence"
all$fireplace_qu[is.na(all$fireplace_qu)] <- "no_fireplace"

#  Basement
all$bsmt_fin_type2[is.na(all$bsmt_fin_type2)] <- "no_basement"
all$bsmt_fin_type1[is.na(all$bsmt_fin_type1)] <- "no_basement"
all$bsmt_exposure[is.na(all$bsmt_exposure)] <- "no_basement"
all$bsmt_qual[is.na(all$bsmt_qual)] <- "no_basement"
all$bsmt_cond[is.na(all$bsmt_cond)] <- "no_basement"

# Garage
all$garage_type[is.na(all$garage_type)] <- "no_garage"
all$garage_finish[is.na(all$garage_finish)] <- "no_garage"
all$garage_qual[is.na(all$garage_qual)] <- "no_garage"
all$garage_cond[is.na(all$garage_cond)] <- "no_garage"


df_missing <- map_df(all, function(x) mean(is.na(x))) %>%
        gather()

ggplot(filter(df_missing, value > 0),
       aes(x = fct_reorder(key, value),
           y = value)) +
        geom_bar(stat = "identity") +
        coord_flip()


```


# EDA
I'll start by exploring the data visually The first thing I do is to look at the distribution of numeric variables. As can be seen in the plot several of the numerical variables are categorical. For those variables I'll probably create full set of dummy variables and remove the zero-variance predictos. According to Kuhn and Johnson this is simple and effective way.  

```{r, fig.width=7, fig.height=8}

all %>% 
        select_if(is.numeric) %>% 
        pivot_longer(cols = 2:36,
                     names_to = "breytur",
                     values_to = "gildi") %>% 
        ggplot(aes(x = gildi)) +
        geom_histogram() +
        facet_wrap(~ breytur, ncol = 6, scales = "free")

```

Since I'm planning to use Elastic Net I might need to transform some of the variables using either Box-Cox or Yeo johnson. Tree-based algorithms can handle skewed distribution. Elastic Net (a linear model with regularization) performance could be improved by transforming relevant variables.

## Correlation among predictos
Before plotting the correlation matrix I have to impute the missing values. I'll use the mice package which uses random forest to imputed the missing values.  
According to the correlation plot there are few variables with high correlation. The year the house was built and the garage has high correlation. As will be discussed below the variable garage_yr_blt will be removed from the dataset.  

```{r}
all_numeric <- all %>%  select_if(is.numeric) 
mice_mod <- mice(all_numeric, method = "rf")
mice_output <- complete(mice_mod)

M <- cor(mice_output)
corrplot(M, method = "circle")

```


## Analyzing missing values
In analyzing missing data I used methods outlined in **Feature Engineering and Selection** by Max Kuhn and Kjell Johnson. You can read the book online here: http://www.feat.engineering/ 
The heatmap below shows that most predictors and samples have nearly complete information.  

```{r}

convert_missing <- function(x) ifelse(is.na(x), 0, 1)

house_missing <- apply(all, 2, convert_missing)

Heatmap(house_missing[sample(nrow(house_missing), 1000), ],
        name = "Missing",
        column_title = "Predictors",
        row_title = "Samles",
        col = c("black", "lightgrey"),
        show_heatmap_legend = FALSE,
        row_names_gp = gpar(fontsize = 0)) # text size for row names

```

Using co-occurance plot can further deepen our understanding on missing information. With this plot we can see if any variables tend to be missing together. For this dataset that doesn't seem to be the case except maybe for mas_vnr_area and mas_vnr_type. Those are only 16 samples out of 2.919. Those will be imputed later using K-nearest neighbors (KNN). Note that I could also use bagged trees to impute missing values. However I will go with KNN here.  

```{r, co_occurr}

gg_miss_upset(all)

```


### lot_frontage
lot_frontage referce to *Linear feet of street connected to property*. When compared to sale_price there doesn't seems to be that much of a difference whether or not the lot_frontage is missing. It could be that the actual value is 0 but we don't know for sure. We could also impute the missing values using KNN and compare it to imputing the missing values with zero.  

#### Numeric variables and missingness of lot_frontage
Here I check if there is any difference between the numeric variables and missingness of lot_frontage. Looking at the graph there doesn't seem to by any notable difference between the numeric variables and the missingness of lot_frontage.  

```{r, lot_front, fig.height=7, fig.width=7}

df_numeric <- all %>%
        select_if(is.numeric) %>%
        mutate(LotFrontage_cat = ifelse(is.na(lot_frontage), "missing", "no_missing")) %>%
        select(-id,- lot_frontage) %>%
        select(LotFrontage_cat, everything())

df_numeric <- df_numeric %>% 
        pivot_longer(cols = 2:ncol(df_numeric),
                     names_to = "key",
                     values_to = "value")

ggplot(df_numeric,
       aes(x = LotFrontage_cat,
           y = value,
           fill = LotFrontage_cat)) +
        geom_violin() +
        facet_wrap(~key, ncol = 6, scales = "free_y") +
        theme(legend.position = "bottom")

```

Next I will train two random forest models to shed light on if I should imputed lot_frontage using KNN or with zeros. As will be discussed below, omitting *garage_yr_blt* is betther than recoding the variable as zero-one. I will do it here when analyzing lot_frontage.  

```{r, lot_frontage_rf}
lot_training <- all %>% filter(id %in% train_id)
lot_training$sale_price <- sale_price


# Impute with zero
lot_zero <- lot_training %>% 
        select(-garage_yr_blt) %>% 
        mutate(lot_frontage = case_when(is.na(lot_frontage) ~ 0,
                                        TRUE ~ lot_frontage)) %>% 
        mutate_if(is.character, as.factor)

lot_train_sample  <- sample(nrow(lot_training), nrow(lot_training)*2/3)
lot_train <- lot_zero[lot_train_sample, ]
lot_test <- lot_zero[-lot_train_sample, ]


lot_zero_rec <- recipe(sale_price ~ ., data = lot_train) %>% 
        step_knnimpute(all_predictors(), neighbors = 5) %>% 
        prep(lot_train)

lot_zero_train <- bake(lot_zero_rec, new_data = lot_train)
lot_zero_test <- bake(lot_zero_rec, new_data = lot_test)


# Impute using KNN
lot_impute <- lot_training %>% 
        mutate_if(is.character, as.factor)

lot_imp_train <- lot_impute[lot_train_sample, ]
lot_imp_test <- lot_impute[-lot_train_sample, ]

lot_impute_rec <- recipe(sale_price ~ ., data = lot_imp_train) %>% 
        step_knnimpute(all_predictors(), neighbors = 5) %>% 
        prep(lot_imp_train)

lot_impute_train <- bake(lot_impute_rec, new_data = lot_imp_train)
lot_impute_test <- bake(lot_impute_rec, new_data = lot_imp_test)

```


```{r, lot_frontage_model}

# Modelling
n_features_lot <- length(setdiff(names(lot_training), "sale_price"))


rf_model_zero <- ranger::ranger(sale_price ~ .,
                               data = lot_zero_train[, -1], # delete the id
                               mtry = floor(n_features_lot / 3),
                               respect.unordered.factors = "order",
                               importance = "impurity")

rf_model_impute <- ranger::ranger(sale_price ~ .,
                               data = lot_impute_train[, -1],  # delete the id
                               mtry = floor(n_features_lot / 3),
                               respect.unordered.factors = "order",
                               importance = "impurity")


pred_lot_zero <- predict(rf_model_zero, data = lot_zero_test)$predictions
pred_lot_impute <- predict(rf_model_impute, data = lot_impute_test)$predictions


RMSE(pred_lot_zero, lot_zero_test$sale_price)
RMSE(pred_lot_impute, lot_impute_test$sale_price)

```

The difference in terms of RMSE on test set is small. I will impute the missing values using KNN.    


### garage_yr_blt
This is a problematic variable. It is numeric and NA if garage is missing. I think imputing the variable is not the right thing to do. Imputing it with zero or the year when the house was build would indicate that there is a garage in the first place, which is not true. Before taking any actions I'll check if this variable is important or not using random forest by first omitting the samples with missing data for garage_yr_blt.  

```{r}
# I have to change all character variables to factor. Got an error: Error in gower_work....

train_no_garageyrblt <- all %>% 
        filter(id %in% train_id)

train_no_garageyrblt$sale_price <- sale_price

train_no_garageyrblt <- train_no_garageyrblt %>% 
               filter(!(is.na(garage_yr_blt))) %>% 
        select(-id) %>% 
        mutate_if(is.character, as.factor)

ames_rf_importance <- recipe(sale_price ~ ., data = train_no_garageyrblt) %>% 
        step_knnimpute(all_predictors(), neighbors = 5) %>% 
        prep(train_no_garageyrblt) %>% 
        bake(train_no_garageyrblt)

n_features <- length(setdiff(names(train_no_garageyrblt), "sale_price"))


rf_model_importance <- ranger::ranger(sale_price ~ .,
                                      data = ames_rf_importance,
                                      mtry = floor(n_features / 3),
                                      respect.unordered.factors = "order",
                                      importance = "impurity")


```


```{r, fig.height=7}
df_rf_importance <- as.data.frame(rf_model_importance$variable.importance) %>% 
        rownames_to_column(var = "variable") %>% 
        as_tibble()

colnames(df_rf_importance) <- c("variable", "importance")
df_rf_importance <- df_rf_importance %>% arrange(desc(importance))

# Finn út hvort breytan sé numeric eða ekki

df_tegund <- map_df(training, function(x) is.numeric(x)) %>% 
        gather("variable", "is_numeric")

df_rf_importance <- left_join(df_rf_importance, df_tegund)

ggplot(df_rf_importance,
       aes(x = fct_reorder(variable, -importance),
           y = importance,
           fill = is_numeric)) +
        geom_bar(stat = "identity") + 
        coord_flip()

rank_garagyrb <- which(df_rf_importance$variable == "garage_yr_blt")
yrblt_garageyrb <- mean(train_no_garageyrblt$year_built == train_no_garageyrblt$garage_yr_blt)

```

The year when the garage is built ranks number `r rank_garagyrb` of `r n_features` variables in the dataset. It is important enought that I don't want to omit it right away. So the next thing I do is to compare RMSE on test set when I omit the variable vs when the variable is present.

```{r}

ggplot(train_no_garageyrblt,
       aes(x = year_built,
           y = garage_yr_blt)) + 
        geom_point()

```


## Omitting vs. recoding
Let's now compare the cross validated accuracy between models when omitting the garage_yr_blt and when the variable is recoded as 1 for garage, and 0 for no garage.  
```{r}

# Omitting
train_gar_omit <- all %>% 
        filter(id %in% train_id)

train_gar_omit$sale_price <- sale_price

train_gar_omit <- train_gar_omit %>% 
        select(-garage_yr_blt) %>% 
        mutate_if(is.character, as.factor)


train_gar_omit_train <- train_gar_omit[1:1000, ]
train_gar_omit_test <- train_gar_omit[1001:nrow(train_gar_omit), ]

ames_rf_omit <- recipe(sale_price ~ ., data = train_gar_omit_train) %>% 
        step_knnimpute(all_predictors(), neighbors = 5) %>% 
        prep(train_gar_omit)

ames_rf_omit_train <- bake(ames_rf_omit, new_data = train_gar_omit_train)
ames_rf_omit_test <- bake(ames_rf_omit, new_data = train_gar_omit_test)



# Recoding
train_gar_recode <- all %>% 
        filter(id %in% train_id)

train_gar_recode$sale_price <- sale_price

train_gar_recode <- train_gar_recode %>% 
        mutate(garage_yr_blt_recode = case_when(is.na(garage_yr_blt) ~ 0,
                                                TRUE ~ 1)) %>%
        select(-garage_yr_blt) %>% 
        mutate_if(is.character, as.factor) %>% 
        filter(id %in% train_gar_omit$id)

train_gar_recode_train <- train_gar_recode[1:1000, ]
train_gar_recode_test <- train_gar_recode[1001:nrow(train_gar_omit), ]


ames_rf_recode <- recipe(sale_price ~ ., data = train_gar_recode_train) %>% 
        step_knnimpute(all_predictors(), neighbors = 5) %>% 
        prep(train_gar_recode)

ames_rf_recode_train <- bake(ames_rf_recode, new_data = train_gar_recode_train)
ames_rf_recode_test <- bake(ames_rf_recode, new_data = train_gar_recode_test)



# Modelling
n_features_omit <- length(setdiff(names(train_gar_omit_train), "sale_price"))
n_features_rec <- length(setdiff(names(train_gar_recode_train), "sale_price"))



rf_model_omit <- ranger::ranger(sale_price ~ .,
                               data = ames_rf_omit_train,
                               mtry = floor(n_features_omit / 3),
                               respect.unordered.factors = "order",
                               importance = "impurity")

rf_model_rec <- ranger::ranger(sale_price ~ .,
                               data = ames_rf_recode_train,
                               mtry = floor(n_features_rec / 3),
                               respect.unordered.factors = "order",
                               importance = "impurity")


pred_omit <- predict(rf_model_omit, data = ames_rf_omit_test)$predictions
pred_rec <- predict(rf_model_rec, data = ames_rf_recode_test)$predictions


RMSE(pred_omit, train_gar_omit_test$sale_price)
RMSE(pred_rec, train_gar_recode_test$sale_price)


```

The difference is small. I will omit the variable form my analysis.


## New data
```{r}
# Recoded
all <- all %>%
        select(-garage_yr_blt)

```


# Analysis of categorical variables
I can either lump rare categories and then create dummy variable or, if the variable is not important according to random forest, I can just recode the variable as zero-one variable. According to the variable importance graph above there are four important categorical variables. I will take a look at those four variables now to see if and how those variable will be lumped. All other categorical variables will be recoded as zero-one variables.  
Note that some of the numeric and categorical
```{r}
df_categorical <- all %>% 
        select_if(is.character)


# number of categories in each variable
df_categorical %>% 
        pivot_longer(cols = 1:ncol(df_categorical),
                     names_to = "key",
                     values_to = "value") %>% 
        group_by(key) %>% 
        summarise(number = n_distinct(value)) %>% 
        ggplot(aes(x = fct_reorder(key, number),
                   y =  number)) +
        geom_bar(stat = "identity") +
        coord_flip()


# Lump mynd
gg_cat_lump <- function(data, x, thres = 0.02) {
        data %>% 
                group_by({{ x }}) %>% 
                summarise(frac = n()/1460) %>% 
                mutate(frac_cat = case_when(frac < thres ~ "lump",
                                        TRUE ~ "no_lump")) %>% 
                ggplot(aes(x = fct_reorder({{ x }}, frac),
                           y = frac,
                           fill = frac_cat)) + 
                geom_bar(stat = "identity") +
                coord_flip()
}

g_box <- function(data, x, thres = 0.02) {
        data %>% 
                group_by({{ x }}) %>% 
                mutate(frac = n()/1460) %>% 
                mutate(frac_cat = case_when(frac < thres ~ "lump",
                                            TRUE ~ "no_lump")) %>% 
                ungroup() %>% 
                ggplot(aes(x = {{x}},
                           y = sale_price,
                       fill = frac_cat)) +
                geom_boxplot()
}

```


## Neighborhood

```{r}
# neighborhood - fraction

gg_cat_lump(training, neighborhood, thres = 0.02)
g_box(training, x = neighborhood, thres = 0.01) + coord_flip()

```

## exter_qual - ordered variable
exter_qual will not be lumped since Ex and Fa would be lumped and the sale price is really different between those two quality categories.  
```{r}
gg_cat_lump(training, exter_qual, thres = 0.02)
g_box(training, exter_qual, thres = 0.02)

```


## bsmt_qual
Using threashold of 0.05 seems to be fine. Simildar distribution for sale_price for categories to be lumped.
```{r}
g1 <- gg_cat_lump(training, bsmt_qual, thres = 0.05)
g2 <- g_box(training, bsmt_qual, thres = 0.05)

gridExtra::grid.arrange(g1, g2)

```


## kitchen_qual
I would need to use threashold = 0.1 to lump Ex and Fa. Those two categories have really different categories so I will not lump.
```{r}

g1 <- gg_cat_lump(training, kitchen_qual, thres = 0.1)
g2 <- g_box(training, kitchen_qual, thres = 0.1)

gridExtra::grid.arrange(g1, g2)

```

## fireplace_qu
Using threashold of 0.05 I lump three categories, Fair (FA), Excellent (Ex) and Poor (Po). I don't think it's a good idea to lump these with such a different distributions. If I have trouble later on I might lump these.
```{r}

g1 <- gg_cat_lump(training, fireplace_qu, thres = 0.02)
g2 <- g_box(training, fireplace_qu, thres = 0.02)

gridExtra::grid.arrange(g1, g2)

```



### Lumping variables
* Neighborhood using threashold of 0.01. With 0.02 the distribution of sale_price is too unequal
* bsmt_qual using 0.05



### Visualisation of numeric variables
```{r}
df_numeric <- training %>% select_if(is.numeric)
```


### Recipe
```{r}
# ames_recipe_knn <- recipe(SalePrice ~ ., data = training) %>% 
#         step_log(all_outcomes()) %>% 
#         step_knnimpute()
```

