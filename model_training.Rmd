---
title: "Untitled"
output:
        github_document:
                toc: true
                fig_width: 7
                fig_height: 7
---

```{r setup, include=FALSE, message=FALSE, warning=TRUE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)

# Modelling
library(tidymodels)
library(caret)
library(caretEnsemble)
# library(bartMachine)


library(doParallel)
library(parallel)

df_training <- read_csv("final_trian.csv")
df_test <- read_csv("final_test.csv")

sale_price <- df_training$sale_price
train_id <- df_training$id
test_id <- df_test$id

df_training <- df_training %>% select(-sale_price)
# df_test <- df_test %>% select(-id)

# Breyti nokkrum factorum í ordered factor

```


# Data preparation
Some of the factor variables are ordered factors. Since write_csv coerce factors to characters I have to fix it here. Some of the variables are categorical variables stored as integers. I will also address that issue here.

Categorical variables that I'll convert to ordered factor:  
* exter_qual
* exter_cond
* bsmt_qual
* bsmt_cond
* heating_qc
* kitchen_qual
* fireplace_qu
* garage_qual
* garage_cond
* pool_qc  

Some of the variables like pool_qc has categories like Good, Average, Fair and NA (not a number) when there is no swimming pool. It's not obvious where NA should be i.e. below Fair or above Excellent. I've decided that it should be at the bottom, i.e. below average. One other remedy to this is to simply not code it as ordered factor. But then you loose information. Strictly I should test both and check which one will yield higher accuracy.  



```{r}

ames_all <- bind_rows(df_training, df_test)

ames_all$exter_qual <- factor(ames_all$exter_qual, levels = c("Fa", "TA","Gd", "Ex"), ordered = TRUE)
ames_all$exter_cond <- factor(ames_all$exter_cond, levels = c("Po", "Fa", "TA" ,"Gd", "Ex"), ordered = TRUE)
ames_all$bsmt_qual <- factor(ames_all$bsmt_qual, levels = c("no_basement", "Fa", "TA", "Gd", "Ex"), ordered = TRUE)
ames_all$bsmt_cond <- factor(ames_all$bsmt_cond, levels = c("no_basement", "Po", "Fa", "TA", "Gd", "Ex"), ordered = TRUE)
ames_all$heating_qc <- factor(ames_all$heating_qc, levels = c("Po", "Fa", "TA", "Gd", "Ex"), ordered = TRUE)
ames_all$kitchen_qual <- factor(ames_all$kitchen_qual, levels = c("Fa", "TA", "Gd", "Ex"), ordered = TRUE)
ames_all$fireplace_qu <- factor(ames_all$fireplace_qu, levels = c("no_fireplace", "Po", "Fa", "TA", "Gd", "Ex"), ordered = TRUE)
ames_all$garage_qual <- factor(ames_all$garage_qual, levels = c("no_garage", "Po", "Fa", "TA", "Gd", "Ex"), ordered = TRUE)
ames_all$garage_cond <- factor(ames_all$garage_cond, levels = c("no_garage", "Po", "Fa", "TA", "Gd", "Ex"), ordered = TRUE)
ames_all$pool_qc <- factor(ames_all$pool_qc, levels = c("no_pool", "Fa", "TA", "Gd", "Ex"), ordered = TRUE)

# Verð að breyta character í factor
ames_all <- ames_all %>% 
        mutate_if(is.character, as.factor)

# Bý til aftur train og test set
ames_training <- ames_all %>% filter(id %in% df_training$id) %>% select(-id)
# ames_training$sale_price <- df_training$sale_price
ames_test <- ames_all %>% filter(id %in% df_test$id) %>% select(-id)

```


```{r}
# Breytur sem ég mun breyta úr integer í factor (unordered)
int_var <- c("bedroom_abv_gr", "bsmt_full_bath", "bsmt_half_bath", "fireplaces", "full_bath", "garage_cars",
             "half_bath", "kitchen_abv_gr", "mo_sold", "tot_rms_abv_grd", "kmeans", "ms_sub_class", "yr_sold")


to_dummy <- c("ms_zoning", "street", "alley", "lot_shape", "land_contour", "utilities", "lot_config",
              "land_slope", "neighborhood", "condition1", "condition2", "bldg_type", "house_style", "roof_style",
              "roof_matl", "exterior1st", "exterior2nd", "mas_vnr_type", "exter_qual", "exter_cond", "foundation",
              "bsmt_qual", "bsmt_cond", "bsmt_exposure", "bsmt_fin_type1", "bsmt_fin_type2", "heating", "heating_qc",
              "central_air", "electrical", "kitchen_qual", "overall_qual", "overall_cond", "functional", "fireplace_qu",
              "garage_type", "garage_finish", "garage_qual", "garage_cond", "paved_drive", "pool_qc", "fence", "misc_feature",
              "sale_type", "sale_condition")

yeo <- c("total_bsmt_sf", "x1st_flr_sf", "lot_area", "bsmt_fin_sf1", "mas_vnr_area", "bsmt_unf_sf",
         "x2nd_flr_sf", "gr_liv_area", "garage_area", "wood_deck_sf", "open_porch_sf")

to_scale <- ames_all %>% names() %>% as_tibble() %>% filter(!(value %in% to_dummy), !(value %in% int_var), value != "id")
to_scale <- to_scale$value


```



# Setup
In data_prep document I analyzed the data visually and by using random forest. I found few categorical variable that I'll have to lump, other that I will change to dummy variables. Missing values will be imputed using KNN.


## Recipe
```{r}

ames_recipe<- recipe(~., data = ames_training) %>% 
        
        # impute
        step_knnimpute(all_predictors()) %>% 
        
        # yeo johnon
        step_YeoJohnson(one_of(yeo)) %>% 
        
        # integer values to factor
        step_num2factor(one_of(int_var)) %>% 
        # ordred factor
        step_num2factor(starts_with("overall_"), ordered = TRUE) %>% 
        
        # lump variables
        step_other(neighborhood, threshold = 0.03) %>% 
        step_other(bsmt_qual, threshold = 0.05) %>% 
        
        # dummy
        step_dummy(one_of(int_var)) %>% 
        step_dummy(one_of(to_dummy)) %>% 
        
        # Interaction
        step_interact(terms = ~ starts_with("overall_"):starts_with("neighborhood_") +
                              starts_with("overall_"):gr_liv_area + 
                              starts_with("overall_"):starts_with("exter_qual_")) %>%
        
        step_interact(terms = ~ starts_with("neighborhood_"):gr_liv_area + 
                              starts_with("neighborhood_"):starts_with("exter_qual_")) %>% 
        step_interact(terms = ~ gr_liv_area:starts_with("exter_qual_")) %>% 
        step_center(one_of(to_scale)) %>% 
        step_scale(one_of(to_scale)) %>% 
        step_knnimpute(all_predictors())

ames_recipe_nzv <- ames_recipe %>% 
        step_zv(all_predictors()) %>% 
        step_nzv(all_predictors())

```



```{r, prep}

trained_rec <- prep(ames_recipe, training = ames_training)
trained_rec_nzv <- prep(ames_recipe_nzv, training = ames_training)

# Full data
train_data <- bake(trained_rec, new_data = ames_training)
test_data <- bake(trained_rec, new_data = ames_test)

# add id and sale_price
train_data$id <- train_id
train_data$sale_price <- sale_price
test_data$id <- test_id


# without nzv
train_data_nzv <- bake(trained_rec_nzv, new_data = ames_training)
test_data_nzv <- bake(trained_rec_nzv, new_data = ames_test)

# add sale_price
train_data_nzv$sale_price <- sale_price


```



# Training
I plan to train several models and then stack them. I'm not sure if 

```{r, model_setup}

ctrl <- trainControl(method = "cv",
                     number = 10,
                     allowParallel = TRUE)

ctrl_2 <- trainControl(method = "repeatedcv",
                       number = 10,
                       repeats = 3)

ctrl_3 <- trainControl(method = "cv",
                       number = 10,
                       allowParallel = TRUE,
                       search = "random")

```


## Elastic net

```{r, enet}

# y <- train_data_nzv$sale_price
# x <- train_data_nzv %>% select(-sale_price)
# 
# no_cores <- detectCores() - 2
# cl <- makeCluster(no_cores)
# registerDoParallel(cl)


byrja <- Sys.time()

enet_train <- train(sale_price ~ .,
                    data = train_data_nzv,
                    method = "glmnet",
                    tuneLength = 20,
                    metric = Metrics::rmsle(),
                    trControl = ctrl)

Sys.time() - byrja

saveRDS(enet_train, "glmnet.rds")

enet_train <- readRDS("glmnet.rds")

```



## MARS

```{r, mars}
# 
# byrja <- Sys.time()
# 
# mars_train <- train(sale_price ~ .,
#                     data = train_data_nzv,
#                     method = "earth",
#                     tuneLength = 10,
#                     metric = "RMSE",
#                     trControl = ctrl)
# 
# 
# Sys.time() - byrja
# 
# saveRDS(mars_train, "mars.rds")

mars_train <- readRDS("mars.rds")

```


## KNN

```{r, knn}
# 
# byrja <- Sys.time()
# 
# knn_train <- train(sale_price ~ .,
#                     data = train_data_nzv,
#                     method = "knn",
#                     tuneLength = 30,
#                     metric = "RMSE",
#                     trControl = ctrl_2)
# 
# Sys.time() - byrja
# 
# saveRDS(knn_train, "knn.rds")

knn_train <- readRDS("knn.rds")

```

## KKNN

```{r}

# no_cores <- detectCores() - 2
# cl <- makeCluster(no_cores)
# registerDoParallel(cl)
# 
# byrja <- Sys.time()
# 
# kknn_train <- train(sale_price ~ .,
#                     data = train_data_nzv,
#                     method = "kknn",
#                     tuneLength = 50,
#                     metric = "RMSE",
#                     trControl = ctrl_3)
# 
# Sys.time() - byrja
# 
# saveRDS(knn_train, "kknn.rds")

```


## Cubist

```{r, cubist}

# cub_grid <- expand.grid(committees = c(1, seq(10, 100, 10)),
#                         neighbors = seq(0, 9, 1))
# 
# byrja <- Sys.time()
# 
# cubist_train <- train(sale_price ~ .,
#                     data = train_data_nzv,
#                     method = "cubist",
#                     tuneGrid = cub_grid,
#                     metric = "RMSE",
#                     trControl = ctrl)
# 
# Sys.time() - byrja
# 
# 
# saveRDS(cubist_train, "cubist.rds")

cubist_train <- readRDS("cubist.rds")

```

## SVM

```{r}

# byrja <- Sys.time()
# 
# svm_train <- train(sale_price ~ .,
#                     data = train_data_nzv,
#                     method = "svmRadial",
#                     tuneLength = 100,
#                     metric = "RMSE",
#                     trControl = ctrl_3)
# 
# 
# Sys.time() - byrja
# 
# saveRDS(svm_train, "svm_train.rds")
# 
# stopCluster(cl)

svm_train <- readRDS("svm_train")

```



## XGBoost
Prófa fyrst search = random með tunelength = 3000 (vissuelga of mikið, hefði verið nóg að nota 100). Með þessu fæ ég góða tilfinning fyrir því hvar ég eigi að focusera 
```{r}

# byrja <- Sys.time()
# 
# no_cores <- detectCores() - 1
# registerDoParallel(cores=no_cores)
# cl <- makeCluster(no_cores)
# 
# xgb_train <- train(sale_price ~ .,
#                     data = train_data_nzv,
#                     method = "xgbTree",
#                     tuneLength = 3000,
#                     metric = "RMSE",
#                     trControl = ctrl_3)
# 
# 
# Sys.time() - byrja
# 
# stopCluster(cl)
# saveRDS(xgb_train, "xgb_random_3000.rds")


xgb_train <- readRDS("xgb_random_3000.rds")

```

## Bartmachine

```{r}
# x_train <- train_data_nzv %>% select(-sale_price) %>% as.data.frame()
# y_train <- train_data_nzv$sale_price
# 
# options(java.parameters = "-Xmx20000m")
# set_bart_machine_num_cores(3)
# 
# bart_machine <- bartMachine(X = x_train, y = y_train)

# bart <- train(sale_price ~ .,
#               data = train_data_nzv,
#               method = "bartMachine",
#               tuneLength = 5,
#               metric = "RMSE",
#               trControl = trainControl(method = "cv", number = 5))
```


# RMSE of different models

```{r}
df_rmse <- tibble(enet = min(enet_train$results[,3]),
                  cubist = min(cubist_train$results[,3]),
                  mars = min(min(mars_train$results[,3])),
                  knn = min(knn_train$results[,2]),
                  svm = min(svm_train$results[,3]),
                  xgb = min(xgb_train$results[,8]))


```


# Stacking
Here I'll use CaretEnsemble to combine several models

```{r, model_list}

no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
registerDoParallel(cl)

my_control <- trainControl(method = "cv",
                           number = 10,
                           allowParallel = TRUE,
                           savePredictions = "final")

x_train <- train_data_nzv %>% select(-sale_price) %>% as.data.frame()
y_train <- train_data_nzv$sale_price


model_list <- caretList(x = x_train,
                        y = y_train,
                        trControl = my_control,
                        continue_on_fail = FALSE,
                        # methodList = c("glmnet", "cubist", "earth",
                        #                "knn", "svmRadial", "xgbTree"),
                        tuneList = list(
                                glmnet = caretModelSpec(method = "glmnet", tuneGrid = data.frame(.alpha = 0.1, .lambda = 1511.753)),
                                cubist = caretModelSpec(method = "cubist", tuneGrid = data.frame(committees = 100, neighbors = 3)),
                                earth = caretModelSpec(method = "earth", tuneGrid = data.frame(nprune = 14, degree = 1)),
                                knn = caretModelSpec(method = "knn", tuneGrid = data.frame(k = 5)),
                                svm = caretModelSpec(method = "svmRadial", tuneGrid = data.frame(sigma = 0.000809491, C = 19.50694)),
                                xgb = caretModelSpec(method = "xgbTree", tuneGrid = data.frame(nrounds = 544, max_depth = 4,
                                                                                               eta = 0.06978232, gamma = 5.719467, 
                                                                                               colsample_bytree = 0.6949411, 
                                                                                               min_child_weight = 2, subsample = 0.7168163))
                        ))


# model_list <- list(glmnet = model_list$glmnet,
#                    cubist = model_list$cubist,
#                    mars = model_list$earth,
#                    knn = model_list$knn,
#                    svm = model_list$svm,
#                    xgb = model_list$xgb)
```


```{r, model_results}

model_results <- data.frame(
 glmnet = min(model_list$glmnet$results$RMSE),
 cubist = min(model_list$cubist$results$RMSE),
 mars = min(model_list$mars$results$RMSE),
 knn = min(model_list$knn$results$RMSE),
 svm = min(model_list$svm$results$RMSE),
 xgb = min(model_list$xgb$results$RMSE)
 )

resamples <- resamples(model_list)

dotplot(resamples, metric = "RMSE")

modelCor(resamples)
```


```{r, ensemble}

ensemble_1 <- caretEnsemble(
        model_list,
        metric = "RMSE",
        trControl = my_control
)

summary(ensemble_1)

ensemble_2 <- caretStack(model_list,
                         method = "glm",
                         metric = "RMSE")

print(ensemble_2)

# 
# ensemble_3 <- caretStack(model_list,
#                          method = "gbm",
#                          metric = "RMSE",
#                          verbose = FALSE,
#                          tuneLength = 10)


```


```{r, prediction}

final_pred <- predict(ensemble_1, newdata = test_data_nzv)


final_prediction <- tibble(ID = test_id,
                           SalePrice = as.numeric(final_pred))

write_csv(final_prediction, "final_prediction.csv")

```



# XGboost raw

```{r}

no_cores <- detectCores() - 1
registerDoParallel(cores=no_cores)
cl <- makeCluster(no_cores)

byrja <- Sys.time()

xgb_train <- train(sale_price ~ .,
                    data = df_training,
                    method = "xgbTree",
                    tuneLength = 10,
                    metric = "RMSE",
                    trControl = ctrl)


Sys.time() - byrja

stopCluster(cl)


```

